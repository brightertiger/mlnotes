<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Support Vector Machines | Notes on ML</title>
  <meta name="description" content="6 Support Vector Machines | Notes on ML" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Support Vector Machines | Notes on ML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Support Vector Machines | Notes on ML" />
  
  
  

<meta name="author" content="brightertiger" />


<meta name="date" content="2022-03-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clustering.html"/>
<link rel="next" href="dimensionality-reduction.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ML Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html"><i class="fa fa-check"></i><b>2</b> Decision Trees &amp; Random Forests</a>
<ul>
<li class="chapter" data-level="2.0.1" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#decision-trees"><i class="fa fa-check"></i><b>2.0.1</b> Decision Trees</a></li>
<li class="chapter" data-level="2.0.2" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#splitting"><i class="fa fa-check"></i><b>2.0.2</b> Splitting</a></li>
<li class="chapter" data-level="2.0.3" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.0.3</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.0.4" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#nature-of-decision-trees"><i class="fa fa-check"></i><b>2.0.4</b> Nature of Decision Trees</a></li>
<li class="chapter" data-level="2.0.5" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#bagging"><i class="fa fa-check"></i><b>2.0.5</b> Bagging</a></li>
<li class="chapter" data-level="2.0.6" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#random-forest"><i class="fa fa-check"></i><b>2.0.6</b> Random Forest</a></li>
<li class="chapter" data-level="2.0.7" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#extratrees"><i class="fa fa-check"></i><b>2.0.7</b> ExtraTrees</a></li>
<li class="chapter" data-level="2.0.8" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#variable-importance"><i class="fa fa-check"></i><b>2.0.8</b> Variable Importance</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>3</b> Boosting</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="boosting.html"><a href="boosting.html#overview"><i class="fa fa-check"></i><b>3.0.1</b> Overview</a></li>
<li class="chapter" data-level="3.0.2" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>3.0.2</b> Gradient Boosting</a></li>
<li class="chapter" data-level="3.0.3" data-path="boosting.html"><a href="boosting.html#adaboost-for-classification"><i class="fa fa-check"></i><b>3.0.3</b> Adaboost for Classification</a></li>
<li class="chapter" data-level="3.0.4" data-path="boosting.html"><a href="boosting.html#notes"><i class="fa fa-check"></i><b>3.0.4</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>4</b> XGBoost</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="xgboost.html"><a href="xgboost.html#mathematical-details"><i class="fa fa-check"></i><b>4.0.1</b> Mathematical Details</a></li>
<li class="chapter" data-level="4.0.2" data-path="xgboost.html"><a href="xgboost.html#regression"><i class="fa fa-check"></i><b>4.0.2</b> Regression</a></li>
<li class="chapter" data-level="4.0.3" data-path="xgboost.html"><a href="xgboost.html#classification"><i class="fa fa-check"></i><b>4.0.3</b> Classification</a></li>
<li class="chapter" data-level="4.0.4" data-path="xgboost.html"><a href="xgboost.html#optimizations"><i class="fa fa-check"></i><b>4.0.4</b> Optimizations</a></li>
<li class="chapter" data-level="4.0.5" data-path="xgboost.html"><a href="xgboost.html#comparisons"><i class="fa fa-check"></i><b>4.0.5</b> Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>5</b> Clustering</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="clustering.html"><a href="clustering.html#hierarchical-agglomerative-clustering"><i class="fa fa-check"></i><b>5.0.1</b> Hierarchical Agglomerative Clustering</a></li>
<li class="chapter" data-level="5.0.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>5.0.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="5.0.3" data-path="clustering.html"><a href="clustering.html#spectral-clustering"><i class="fa fa-check"></i><b>5.0.3</b> Spectral Clustering</a></li>
<li class="chapter" data-level="5.0.4" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>5.0.4</b> DBSCAN</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>6</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linear-svm"><i class="fa fa-check"></i><b>6.0.1</b> Linear SVM</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>7</b> Dimensionality Reduction</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#background"><i class="fa fa-check"></i><b>7.0.1</b> Background</a></li>
<li class="chapter" data-level="7.0.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principal-component-analysis"><i class="fa fa-check"></i><b>7.0.2</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="7.0.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#stochastic-neighbour-embedding-sne"><i class="fa fa-check"></i><b>7.0.3</b> Stochastic Neighbour Embedding (SNE)</a></li>
</ul></li>
<li class="divider"></li>
<li>Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on ML</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-machines" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Support Vector Machines</h1>
<div id="linear-svm" class="section level3" number="6.0.1">
<h3><span class="header-section-number">6.0.1</span> Linear SVM</h3>
<ul>
<li>Classification setting</li>
<li>Find the maximum-margin hyperplane that can separate the data</li>
<li>Best hyperplane is the one that maximizes the margin
<ul>
<li>Margin the distance of the hyperplane to closest data points from both classes</li>
<li>Hyperplane: <span class="math inline">\(H : wx +b = 0\)</span></li>
</ul></li>
<li>Distance of a point (x) to a hyperplane (h):
<ul>
<li><span class="math inline">\(d = \frac{|Wx + b|}{||W||^2}\)</span></li>
</ul></li>
<li>Margin is defined by the point closest to the hyperplane
<ul>
<li><span class="math inline">\(\gamma(W,b) = \min_{x \in D} \frac{|Wx + b|}{||W||^2}\)</span></li>
<li>Margin is scale invariant</li>
</ul></li>
<li>SVM wants to maximize this margin
<ul>
<li>For margin to be maximized, hyperplane must lie right in the middle of the two classes</li>
<li>Otherwise it can be moved towards data points of the class that is further away and be further increased</li>
</ul></li>
<li>Mathematics
<ul>
<li>Binary Classification
<ul>
<li><span class="math inline">\(y_i \in \{+1,-1\}\)</span></li>
</ul></li>
<li>Need to find a separating hyperplane such that
<ul>
<li><span class="math inline">\((Wx_i + b) &gt; 0 \; \forall \; y_i = +1\)</span></li>
<li><span class="math inline">\((Wx_i + b) &lt; 0 \; \forall \; y_i = -1\)</span></li>
<li><span class="math inline">\(y_i(Wx_i + b) \ge 0\)</span></li>
</ul></li>
<li>SVM posits that the best hyperplane is the one that maximizes the margin
<ul>
<li>Margin acts as buffer which can lead to better generalization</li>
</ul></li>
<li>Objective
<ul>
<li><span class="math inline">\(\max_{W,b} \gamma(W,b) \; \text{subject to} \; y_i(Wx_i + b) &gt; 0\)</span></li>
<li><span class="math inline">\(\max_{W,b} \min_{x \in D} \frac{|Wx + b|}{||W||^2} \; \text{subject to} \; y_i(Wx_i + b) &gt; 0\)</span></li>
<li>A max-min optimization problem</li>
</ul></li>
<li>Simplification
<ul>
<li>The best possible hyperplace is scale invariant</li>
<li>Add a constraint such that <span class="math inline">\(|Wx +b| = 1\)</span></li>
</ul></li>
<li>Updated objective
<ul>
<li><span class="math inline">\(\max \frac{1}{||W||^2} \; \text{subject to} \; y_i(Wx_i + b) \ge 0 \; ; |Wx +b| = 1\)</span></li>
<li><span class="math inline">\(\min ||W||^2 \; \text{subject to} \; y_i(Wx_i + b) \ge 0 \; ; |Wx +b| = 1\)</span></li>
</ul></li>
<li>Combining the contraints
<ul>
<li><span class="math inline">\(y_i(Wx_i + b) \ge 0\; ; |Wx +b| = 1 \implies y_i(Wx_i + b) \ge 1\)</span></li>
<li>Holds true because the objective is trying to minimize W</li>
</ul></li>
<li>Final objective
<ul>
<li><span class="math inline">\(\min ||W||^2 \; \text{subject to} \; y_i(Wx_i + b) \ge 1\)</span><br />
</li>
</ul></li>
<li>Quadratic optimization problem
<ul>
<li>Can be solved quickly unlike regression which involves inverting a large matrix</li>
<li>Gives a unique solution unlike perceptron</li>
</ul></li>
<li>At the optimal solution, some training points will lie of the margin
<ul>
<li><span class="math inline">\(y_i(Wx_i + b) = 1\)</span></li>
<li>These points are called support vectors</li>
</ul></li>
</ul></li>
<li>Soft Constraints
<ul>
<li>What if the optimization problem is infeasible?
<ul>
<li>No solution exists</li>
</ul></li>
<li>Add relaxations i.e.Â allow for some misclassification
<ul>
<li>Original: <span class="math inline">\(y_i(Wx_i + b) \ge 1\)</span></li>
<li>Relaxed: <span class="math inline">\(y_i(Wx_i + b) \ge 1 - \xi_i \; ; \xi_i &gt; 0\)</span></li>
<li><span class="math inline">\(\xi_i = \begin{cases} 1 - y_i(Wx_i + b), &amp; \text{if } y_i(Wx_i + b) &lt; 1\\0, &amp; \text{otherwise} \end{cases}\)</span></li>
<li>Hinge Loss <span class="math inline">\(\xi_i = \max (1 - y_i(Wx_i + b), 0)\)</span></li>
</ul></li>
<li>Objective: <span class="math inline">\(\min ||W||^2 + C \sum_i \max (1 - y_i(Wx_i + b), 0)\)</span>
<ul>
<li>C is the regularization parameter that calculates trade-off</li>
<li>High value of C allows for less torelance on errors</li>
</ul></li>
</ul></li>
<li>Duality
<ul>
<li>Primal problem is hard to solve</li>
<li>Convert the problem to a Dual, which is easier to solve and also provides near-optimal solution to primal</li>
<li>The gap is the optimality that arises in this process is the duality gap</li>
<li>Lagrangian multipliers determine if strong suality exists</li>
<li>Convert the above soft-margin SVM to dual via Lagrangian multipliers</li>
<li><span class="math inline">\(\sum \alpha_i + \sum\sum \alpha_i \alpha_j y_i y_j x_i^T x_j\)</span></li>
<li><span class="math inline">\(\alpha\)</span> is the Lagrangian multiplier</li>
</ul></li>
<li>Kernelization
<ul>
<li>Say the points are not separable in lower dimension
<ul>
<li>Transform them via kernels to project them to a higher dimension</li>
<li>The points may be separable the higher dimension</li>
<li>Non-linear feature transformation</li>
<li>Solve non-linear problems via Linear SVM</li>
</ul></li>
<li>Polynomial Kernel
<ul>
<li><span class="math inline">\(K(x_i, x_j) = (x_i^T x_j + c)^d\)</span></li>
<li>The d regers to the degree of the polynomial</li>
<li>Example: 2 points in 1-D (a and b) transformerd via second order polynomial kernel
<ul>
<li><span class="math inline">\(K(a,b) = (ab + 1)^2 = 2ab+ a^2b^2 + 1 = (\sqrt{2a}, a, 1)(\sqrt{2b}, b, 1)\)</span></li>
</ul></li>
<li>Calculates similarity between points in higher dimension</li>
</ul></li>
<li>RBF Kernel
<ul>
<li><span class="math inline">\(K(x_i, x_j) = \exp \{\gamma |x_i - x_j|^2\}\)</span></li>
<li>The larger the distance between two observations, the less is the similarity</li>
<li>Radial Kernel determines how much influence each observation has on classifying new data points<br />
</li>
<li>Transforms points to an infinite dimension space
<ul>
<li>Tayloy Expansion of exponential term shows how RBF is a polynomial function with inifnite dimensions</li>
</ul></li>
<li>2 points in 1-D (a and b) transformerd via RBF
<ul>
<li><span class="math inline">\(K(a,b) = (1, \sqrt{\frac{1}{1!}}a, \sqrt{\frac{1}{2!}}a^2...)(1, \sqrt{\frac{1}{1!}}b, \sqrt{\frac{1}{2!}}b^2...)\)</span></li>
</ul></li>
</ul></li>
<li>Kernel Trick
<ul>
<li>Transforming the original dataset via Kernels and training SVM is expensive</li>
<li>Convert Dot-products of support vectors to dot-products of mapping functions</li>
<li><span class="math inline">\(x_i^T x_j \implies \phi(x_i)^T \phi(x_j)\)</span></li>
<li>Kernels are chosen in a way that this is feasible</li>
</ul></li>
</ul></li>
<li>SVM For Regression
<ul>
<li>Margins should cover all data points (Hard) or most data points (Soft)</li>
<li>The boundary now lies in the middle of the margins
<ul>
<li>The regression model to estimate the target values</li>
</ul></li>
<li>The objective is to minimize the the distance of the points to the boundary</li>
<li>Hard SVM is sensitive to outliers</li>
</ul></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dimensionality-reduction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
