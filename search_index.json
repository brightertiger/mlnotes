[["index.html", "Notes on ML 1 About", " Notes on ML brightertiger 2022-03-28 1 About Hi I am brightertiger! I am a data scientist looking to expand my knowledge about ML. This is a collection of notes on machine learning concepts based mostly on Statquest along with a few other resources. "],["decision-trees-random-forests.html", "2 Decision Trees &amp; Random Forests", " 2 Decision Trees &amp; Random Forests 2.0.1 Decision Trees Recursively split the input / feature space using stubs i.e. decision rules Splits are parallel to the axis Mathematical Represenation \\(R_j = \\{ x : d_1 &lt;= t_1, d_2 &gt;= t_2 ... \\}\\) \\(\\hat Y_i = \\sum_j w_j I\\{x_i \\in R_j\\}\\) \\(w_j = \\frac{\\sum_i y_i I \\{x_i \\in R_j\\}}{\\sum_i I \\{x_i \\in R_j\\}}\\) Types of Decision Trees Binary Splits Classification and Regression Trees (CART) C4.5 Multiple Splits: CHAID (Chi-Square Automatic Interaction Detection) ID3 2.0.2 Splitting Split Criteria for Classification Trees The nodes are split to decrease inpurity in classification Gini Criterion \\(1 - \\sum_C p_{i}^2\\) Probability that observation belongs to class i: \\(p_i\\) Misclassification: For a given class (say i): \\(p_i \\times p_{k \\ne i} = p_i \\times (1 - p_i)\\) Across all classes: \\(\\sum_C p_i \\times (1 - p_i)\\) \\(\\sum_C p_i - \\sum_c p_{i}^2\\) \\(1 - \\sum_c p_{i}^2\\) Ranges from (0, 0.5) Entropy Criterion Measure of uncertainly of a random variable Given an event E p(E) = 1 \\(\\implies\\) No Surprise p(E) = 0 \\(\\implies\\) Huge Surprise Informaion Content: \\(I(E) = \\log(1 / p(E))\\) Entropy is the expectation of this information content \\(H(E) = - \\sum p(E) \\log(p(E))\\) Maximum when all outcomes have same probability of occurance Ranges from (0, 1) Split Criteria for Regression Trees Sum-Squared Error \\(\\sum_i (Y_i - \\bar Y)^2\\) Finding the Split For any candidate value: Calculate the weighted average reduction in impurity / error Weights being the number of observations flowing in the child nodes Starting Gini \\(\\text{Gini}_{\\text{Root}}\\) \\(N_{\\text{Root}}\\) After Split Child Nodes \\(\\text{Gini}_{\\text{Left}}, N_{\\text{Left}}\\) \\(\\text{Gini}_{\\text{Right}}, N_{\\text{Right}}\\) Updated Gini \\(\\frac{N_{\\text{Left}}}{N_{\\text{Root}}} \\times \\text{Gini}_{\\text{Left}} + \\frac{N_{\\text{Right}}}{N_{\\text{Root}}} \\times \\text{Gini}_{\\text{Right}}\\) Find the split, the results in minimum updated Gini Updated Gini &lt;= Starting Gini Greedy algorithms to find the best splits 2.0.3 Bias-Variance Trade-off Bias Measures ability of an ML algorithm to model true relationship between features and target Simplifying assumptions made by the model to learn the relationship Example: Linear vs Parabolic relationship Low Bias: Less restrictive assupmtions High Bias: More restrictive assumptions Variance The difference in model performance across different datasets drawn from the same distribution Low Variance: Small changes to model perforamance with changes in datasets High Variance: Large changes to model perforamance with changes in datasets Irreducible Error Bayes error Cannot be reduced irrespective of the model form Best model minimizes: \\(\\text{MSE} = \\text{bias}^2 + \\text{variance}\\) Decision trees have low bias and high variance Decision trees are prone to overfitting Noisy Samples Small data samples in nodes down the tree Tree Pruning solves for overfitting Adding a cost term to objetive which captures tree complexity \\(\\text{Tree Score} = SSR + \\alpha T\\) As the tree grows in size, the reduction in SSR has to more than offset the complexity cost 2.0.4 Nature of Decision Trees Decision Trees can model non-linear relationships (complex deicison boundaries) Spline regressions cannot achieve the same results Spline adds indicator variables to capture interactions and create kinks But the decision boundary has to be continuous The same restriction doesn’t apply to decision trees Decision Trees don’t require feature sscaling Decision Trees are less sensitive to outliers Outliers are of various kinds: Outliers: Points with extreme values Input Features Doesn’t impact Decision Trees Split finding will ignore the extreme values Output / Target Influential / High-Leverage Points: Undue influence on model Decision Trees cannot extrapolate well to ranges outside the training data Decision trees cannot capture linear time series based trends / seasonality 2.0.5 Bagging Bootstrap Agrregation Sampling with repetition Given Dataset of Size N Draw N samples with replacement Probability that a point (say i) never gets selected \\((1 - \\frac{1}{N})^N \\approx \\frac{1}{e}\\) Probability that a point (say i) gets selected atleast once \\(1 - \\frac{1}{e} \\approx 63\\%\\) 2.0.6 Random Forest Use bootstrap aggregation (bagging) to create multiple datasets “Random” subspace of dataset Use subset of variables for split at each node sqrt for classification m//3 for regression Comparison to single decision tree Bias remains the same Variance decreases Randomness in data and slpits reduces the correlation in prediction across trees Let \\(\\hat y_i\\) be the prediction from ith tree in the forest Let \\(\\sigma^2\\) be the variance of \\(\\hat y_i\\) Let \\(\\rho\\) be the correlation between two trees in the forest \\(V(\\sum_i \\hat y_i) = \\sum V(\\hat y_i) + 2 \\sum\\sum COV(\\hat y_i, \\hat y_j)\\) \\(V(\\sum_i \\hat y_i) = n \\sigma^2 + n(n-1) \\rho \\sigma^2\\) \\(V( \\frac{1}{n} \\sum_i \\hat y_i) = \\rho \\sigma^2 + \\frac{1-\\rho}{n} \\sigma^2\\) Variance goes down as more trees are added, but bias stays put Output Combination Majority Voting for Classification Averaging for Regression Out-of-bag (OOB) Error Use the non-selected rows in bagging to estimate model performance Comparable to cross-validaiton results Proximity Matrix Use OOB observations Count the number of times each pair goes to the same terminal node Identifies observations that are close/similar to each other 2.0.7 ExtraTrees Extremely Randomized Trees Bagging: ExtraTrees: No Extremely Randomized Trees: Yes Mutiple trees are built using: Random variable subset for splitting Random threshold subsets for a variable for splitting 2.0.8 Variable Importance Split-based importance If variable j is used for split Calculate the improvement in Gini at the split Sum this improvement across all trees and splits wherever jth variable is used Alternate is to calculate the number of times variable is used for splitting Biased in favour of continuous variables which can be split multiple times Permutation-based importance / Boruta Use OOB samples to calculate variable importance Take bth tree: Pass the OOB samples and calculate accuracy Permuate jth variable and calculate the decrease in accuracy Average this decrease in accuracy across all trees to calculate variable importance for j Effect is simialr to setting the coefficient to 0 in regression Takes into account if good surrogates are present in the dataset Partial Dependence Plots Marginal effect of of a feature on target Understand the relationship between feature and target Assumes features are not correlated \\(\\hat f(x_s) =\\frac{1}{C} \\sum f(x_s,x_i)\\) Average predictions over all other variables Can be used to identify important interactions Friedman’s H Statistic If features don’t interact Joint PDP can be decomposed into marginals Shapely Values Model agnositc feature importance LIME "],["boosting.html", "3 Boosting", " 3 Boosting 3.0.1 Overview Combine multiple rules of thumb to make an accurate and informed decision Bagging: Models are buit in parallel on different data subsets Boosting: Models are built in sequence with modified different samples weights \\(F(x_i) = \\sum_m \\alpha_m f_m(x_i)\\) \\(f_m\\) and \\(\\alpha_m\\) are fit jointly PAC Learning Framework Probably Approximately Correct Is the problem learnable? Model has error \\(&lt; \\epsilon\\) with probability \\(&gt; (1 -\\delta)\\) An algorithm that satisfies the PAC thresholds is a strong learner Strong learners are complex models with many parameters and require a lot of training data Weak learners are algorithms that perform slightly better than random guessing Schapire: Strength of Weak Learnability If a problem can be solved by strong learner, it can be solved by a collection of weak learners. Hypothesis boosting mechanism Construct three hypotheses, trained on different data subsets H1: Complete Data H2: Balanced Sampling of correct and incorrect predictions from H1 H3: Disagreements between H1 and H2 predictions Scoring: Majority Voting of H1, H2 and H3 Improved performance but cannot be scaled easily Adaboost - Adaptive Boosting Additive Model Contruct many hypothesis (more than three) The importance/weight of each new hypotheses added “adapts” or changes \\(\\alpha_m = \\frac{1}{2}\\log\\lbrack \\frac{1-\\epsilon_m}{\\epsilon_m} \\rbrack\\) \\(\\epsilon_m\\) si the weighted classification error Every sample has a weight associated while constructing a weak hypothesis Exponential Weighting scheme Correctly Classifier: \\(w_i = w_i \\times \\exp^{\\alpha}\\) Incorrectly Classifier: \\(w_i = w_i \\times \\exp^{-\\alpha}\\) Underfitting: Not enough hypothesis added to ensemble Overfitting: Not using weak learners as hypothesis Gradient Boosting Uses gradients of the loss function to compute the weights Gradients are a proxy of how poorly a data point is classified Adaboost is a special case of gradient boosting 3.0.2 Gradient Boosting Boosting paradigm extended to general loss functions Beyond squared and exponential loss Any loss function that’s differentiable and convex Gradient Descent + Boosting Derivation \\(F(x_i) = \\sum_m \\alpha_m f_m(x_i)\\) \\(f_m(x_i) = \\arg \\min_{f \\in H} L(F(x_i) + \\alpha f_m(x_i))\\) This optimization is analogous to gradient descent in functional space Taylor Approximation \\(\\min L(F(x_i) + \\alpha f_m(x_i))\\) \\(\\min L(F(x_i)) + &lt;\\alpha f_m(x_i), \\frac{\\delta L}{\\delta F} &gt;\\) The first term is constant The second term is inner product over two functions \\(\\min &lt;\\alpha f_m(x_i), \\frac{\\delta L}{\\delta F} &gt;\\) Only interested in the behaviour of these function over training data Evaluate this functions at different points in training data Take the inner product \\(\\min \\sum_i \\frac{\\delta L}{\\delta F(x_i)} \\times \\alpha f(x_i)\\) Pseudo-Residual \\(-\\frac{\\delta L}{\\delta F(x_i)}\\) \\(\\min - \\sum_i r_i \\times \\alpha f(x_i)\\) The ensemble makes improvement as long as \\(\\sum_i r_i f(x_i) &lt; 0\\) Modifications for CART: Using CART as weak learners The minimization problem from Taylor approx can’t be directly optimized by CART Need to modify this to a functional form that can be easily handled (squared loss) \\(r_i\\) is independent of \\(f_m\\), hence \\(\\sum r_i ^2\\) is a constant \\(\\sum \\alpha f_m (x_i) ^2\\) can also be treated as a constant Scale factor to restrict the predictions to certain range \\(\\min \\sum r_i ^2 -2 \\sum_i r_i \\times \\alpha f(x_i) + \\sum \\alpha f_m (x_i) ^2\\) \\(\\min \\sum (r_i - \\alpha f(x_i))^2\\) This squared-loss can be minimized by CART easily Optimal value of \\(\\alpha\\) via Line Search \\(L = \\sum (r_i - \\alpha f(x_i))^2\\) \\(\\alpha^* = \\frac{\\sum r_i f(x_i)}{\\sum f(x_i)^2} \\approx 1\\) Algorithm Given Data \\(\\lbrace x_i, y_i \\rbrace\\) Loss Function \\(L(y_i, F(x_i))\\) Initialize the model with a constant value \\(\\min L(y_i, \\gamma)\\) Compute the pseudo residual \\(r_{im} = -\\frac{\\delta L(y_i, F(x_i))}{\\delta F(x_i)}\\) Build the new weak learner on pseudo residuals Say a decision tree \\(\\gamma_{jm} = \\arg\\min \\sum_{x_\\in R_{ij}} L(y_i, F_m(x_i) + \\gamma)\\) Optimal \\(\\gamma_{jm}\\) value is the average of residuals in the leaf node j Only in case of squared loss L in regression setting Update the ensemble \\(F_{m+1}(x_i) = F_m(x_i) + \\nu \\sum_j \\gamma_{jm} I(x_i \\in R_{jm})\\) \\(\\nu\\) is the step size or shrinkage It prevents overfitting 1st order Taylor approximation works only for small changes Extension to Classification Build a weak learner to predict log-odds Log Odds to Probability: \\(p = \\frac{e^{\\log(odds)}}{1+ e^{\\log(odds)}}\\) Objective is to minimize Negative Log-Likelihood \\(NLL = - \\sum y_i \\log(p_i) + (1 - y_i) \\log(1-p_i)\\) \\(NLL = - \\sum y_i \\log(\\frac{p_i}{1-p_i}) + log(1-p_i)\\) \\(NLL = - \\sum y_i \\log(odds) - \\log(1 + \\exp^{\\log(odds)})\\) Compute Psuedo Residuals \\(\\frac{\\delta NLL}{\\delta \\log(odds)}\\) \\(r_{im} = p_i - y_i\\) Algorithm Given Data \\(\\lbrace x_i, y_i \\rbrace\\) Loss Function \\(L(y_i, F(x_i))\\) Initialize the model with a constant value Log-Odds that minimizes NLL \\(\\min L(y_i, \\gamma)\\) Calculate Psuedo Residuals \\(r_{im} = p_i - y_i\\) Build the new weak learner on pseudo residuals \\(\\gamma_{jm} = \\arg \\min \\sum_{x_\\in R_{ij}} L(y_i, F_m(x_i) + \\gamma)\\) Minimizing this function not easy Use 2nd order Taylor Approximation - \\(\\min L(y_i, F(x_i) + \\gamma) = C + \\gamma \\frac{dL}{dF} + {1 \\over 2}\\gamma^2 \\frac{d^2L}{dF^2}\\) \\(\\gamma^* = - \\frac{dL}{dF} / \\frac{d^2L}{dF^2}\\) \\(\\frac{dL}{dF} = p_i - y_i\\) \\(\\frac{d^2L}{dF^2} = p_i (1 - p_i)\\) \\(\\gamma^* = \\frac{p_i - y_i}{p_i (1 - p_i)}\\) Update the ensemble \\(F_{m+1}(x_i) = F_m(x_i) + \\nu \\sum_j \\gamma_{jm} I(x_i \\in R_{jm})\\) 3.0.3 Adaboost for Classification Additively combines many weak learners to make classifications Adaptively re-weights incorrectly classified points Some weak learners get more weights in the final ensemble than others Each subsequent learner accounts for the mistakes made by the previous one Uses exponential loss \\(y \\in \\{-1,1\\}\\) \\(L(y_i, f(x_i)) = \\exp^{-y_i f(x_i)}\\) Upper bound on 0-1 loss, same as logistic loss Rises more sharply than logistic loss in case of wrong predictions LogitBoost minimizes logistic loss \\(\\log(1 + \\exp^{-y_i f(x_i)})\\) Objective Function Additive Ensemble: \\(F(x) = \\sum_m \\alpha_j f_j(x)\\) Loss: \\(L = \\sum_i \\exp^{-\\frac{1}{2} y_i \\times F(x)}\\) At mth round: \\(L = \\sum_i \\exp^{- \\frac{1}{2} y_i \\times \\sum_m \\alpha_m f_m(x)}\\) \\(L = \\sum_i \\exp^{-\\frac{1}{2} y_i \\times \\sum_{m-1} \\alpha_j f_j(x)} \\times \\exp^{- \\frac{1}{2} y_i \\alpha_m f_m(x_i)}\\) Assume all the values till m-1 as constant \\(L = \\sum_i w^m_i \\times \\exp^{- \\frac{1}{2} y_i \\alpha_m f_m(x_i)}\\) Minimizie E wrt to \\(\\alpha_m\\) to find the optimal value \\(L = \\sum_{corr} w^m_i \\exp^{- \\frac{1}{2} \\alpha_m} + \\sum_{incorr} w^m_i \\exp^{ \\frac{1}{2} \\alpha_m}\\) Assuming \\(\\epsilon_m\\) as the weighted misclassification error \\(L = \\epsilon_m \\exp^{\\frac{1}{2} \\alpha_m} + (1-\\epsilon_m) \\exp^{- \\frac{1}{2} \\alpha_m}\\) Optimal value of \\(\\alpha_m^* = \\frac{1}{2}\\log\\lbrack \\frac{1-\\epsilon_m}{\\epsilon_m} \\rbrack\\) Algorithm Initialization: Give equal weights to all observations For next m rounds: Fit a weak learner Calculate weighted error \\(\\epsilon_m\\) \\(\\epsilon_m = \\frac{\\sum_i w_i^m I(y_i \\ne f_m(x_i))}{\\sum_i w_i^m}\\) Calculate the weight of the new weak learner \\(\\alpha_m = \\frac{1}{2}\\log\\lbrack \\frac{1-\\epsilon_m}{\\epsilon_m} \\rbrack\\) Update the sample weights \\(w_i^{m+1} = w_i^{m} \\times \\exp^{\\alpha^m \\times I(y_i \\ne f_m(x_i))}\\) Normalize Scale factor \\(2 \\sqrt{\\epsilon(1-\\epsilon)}\\) Can be modified to work with regression problems 3.0.4 Notes Gradient boosting uses weak learners which have high bias and low variance and gradually reduces the bias over the ensemble by sequentially combining these weak learners Chronology: Adaboost Adaboost as gradient descent Generalize adaboost to any gradient descent Difference between Gradient Descent and Gradient Boosting In gradient descent, the gradients are used to update parameters of the model In gradient boosting, the gradients are used to build new models Gradient boosting is a meta model that combines weak learners "],["xgboost.html", "4 XGBoost", " 4 XGBoost Extreme Gradient Boosting Introduces regularization to reduce overfitting 4.0.1 Mathematical Details Loss Function \\(L(y_i, p_i)\\) MSE \\({1 \\over 2}\\sum{(y_i - p_i)^2}\\) NLL Loss \\(- \\sum {y_i \\log p_i + (1 - y_i) \\log (1 -p_i)}\\) In XGBoost, the objective has regularization terms \\(\\sum_i L(y_i, p_i) + \\gamma T + {1 \\over 2} \\lambda O_{value}^2\\) \\(O_{value}\\) is the prediction from tree (terminal value in leaf nodes) \\(p_i = p_i^0 + O_{value}\\) \\(p_i^0\\) is the initital prediction / prediction from previous round High values of \\(\\lambda\\) will push the optimal output values close to 0 Second-order Taylor approximation to simplify the objective \\(L(y_i, p_i^0 + O_{value})\\) \\(L(y_i, p_i^0) + \\frac{dL}{dO_{value}} O_{value} + {1 \\over 2} \\frac{d^2L}{dO_{value}^2} O_{value}^2\\) \\(L(y_i, p_i^0) + g O_{value} + {1 \\over 2} H O_{value}^2\\) \\(L(y_i, p_i^0)\\) is constant \\(\\sum_i L(y_i, p_i) = \\sum_i g_i O_{value} + {1 \\over 2} \\sum H_i O_{value}^2\\) Objective Function \\(\\sum_i L(y_i, p_i) + \\gamma T + {1 \\over 2} \\lambda O_{value}^2\\) \\(\\sum_i g_i O_{value} + \\gamma T + {1 \\over 2} (\\sum H_i + \\lambda) O_{value}^2\\) Optimal output value Differentiate objective function wrt \\(O_{value}\\) \\(O_{value}^* = - \\frac{\\sum g_i}{\\sum H_i + \\lambda}\\) For MSE: \\(g_i = - (y_i - p_i^0)\\) \\(H_i = 1\\) For NLL Output value is log(odds) \\(g_i = - (y_i - p_i)\\) \\(H_i = p_i (1 - p_i)\\) Splitting Criteria Objective value at optimal output \\(\\sum_i g_i O_{value} + \\gamma T + {1 \\over 2} (\\sum H_i + \\lambda) O_{value}^2\\) \\({1 \\over 2}{\\sum_i g_i^2 \\over \\sum H_i + \\lambda} + \\gamma T\\) 4.0.2 Regression Calculate similarity score \\(G^2 / (H + \\lambda)\\) \\(\\lambda\\) is the regularization parameter Reduces sensitivity to a particular observation Large values will result in more pruning (shrinks similarity scores) In case of MSE loss function \\(\\sum_i r_i^2 / (N + \\lambda)\\) \\(r\\) is the residual \\(N\\) is the number of observations in the node Calculate Gain for a split \\(\\mathrm{Gain} = \\mathrm{Similarity_{left}} + \\mathrm{Similarity_{right}} - \\mathrm{Similarity_{root}}\\) Split criterion \\(\\mathrm{Gain} - \\gamma &gt; 0\\) \\(\\gamma\\) controls tree complexity Helps prevent over fitting Setting \\(\\gamma = 0\\) doesn’t turn-off pruning Pruning Max-depth Cover / Minimum weight of leaf node N for regression Trees are grown fully before pruning If a child node satisfies minimum Gain but root doesn’t, the child will still exist Output Value of Tree \\(\\sum_i r_i / (N + \\lambda)\\) Output Value of Ensemble Initial Prediction + \\(\\eta\\) Output Value of 1st Tree …. Initial prediction is the simple average of target \\(\\eta\\) is the learning rate 4.0.3 Classification Calculate similarity score \\(G^2 / (H + \\lambda)\\) In case of Log loss function \\(\\sum r_i^2 / (\\sum{p_i (1-p_i)} + \\lambda)\\) \\(r\\) is the residual \\(p\\) is the previous probability estimate Calculate Gain for a split \\(\\mathrm{Gain} = \\mathrm{Similarity_{left}} + \\mathrm{Similarity_{right}} - \\mathrm{Similarity_{root}}\\) Split criterion \\(\\mathrm{Gain} - \\gamma &gt; 0\\) Pruning Max Depth Cover / Minimum weight of leaf node \\(\\sum{p_i (1-p_i)}\\) Output Value of Tree \\(\\sum r_i / (\\sum{p_i (1-p_i)} + \\lambda)\\) Output Value of Ensemble Intial prediction Simple average of target Convert the value to log(odds) Initial Prediction + \\(\\eta\\) Output Value of 1st Tree …. Output is log(odds) Transform the value to probability 4.0.4 Optimizations Approximate Greedy Algorithm Finding splits faster Histogram based splits by bucketing the variables Quantile Sketch Algorithm Approximately calculate the quantiles parallely Quantiles are weighted by cover / hessian Sparsity Aware Split Finding Calculate the split based on known data values of the variable For missing data: Send the observations to left node and calcluate the Gain Send the observations to right node and calcluate the Gain Evaluate which path gives maximum Gain Cache Aware Access Stores gradients and hessians in Cache Compress the data and store on hard-drive for faster access 4.0.5 Comparisons XGBoost Stochastic Gradient Boosting No Treatment for categorical variables Depth-wise tree growth LightGBM Gradient One-Side Sampling (GOSS) Maximum Gradient Observation are oversampled Encoding for categorical variables Exclusive Feature Bundling to reduce number of features Histrogram based splitting Leaf-wise tree growth CatBoost Minimum Variance Sampling Superior encoding technniques for categorical variables Target encoding Symmetric tree growth "],["clustering.html", "5 Clustering", " 5 Clustering Unsupervised learning technique Assign similar data points to a single group / cluster 5.0.1 Hierarchical Agglomerative Clustering At each step, merge the two most similar groups Keep giong unit there is a single group left Similarity between groups Single Link: Distance between the two closest members of each group Complete Link: Distance between the two farthest members of each group Average Link: Average Diatnace between all pairs 5.0.2 K-Means Clustering Hierarchical Clustering is very slow Algorithm Assume there are K (hyperparameter) clusters Assign each data point to it’s nearest cluster center \\(z_n^* = \\arg \\min ||x_n - \\mu_k ||^2\\) Update the cluster centers at the end of assignments \\(\\mu_k = {1 \\over N_k}\\sum_{n, z_n=k} x_n\\) Objective Minimize distortion \\(L = \\sum_{n} ||x_n - \\mu_{z_n}||^2\\) Non-Convex objective, sensitive to intialization Multiple Restarts to control randomness K-Means++ Algorithm Pick centers sequentially to cover the data Pick initial points randomly For subsequent rounds, initialize with points picked with probability proportional to the distance from it’s cluster center Points far away from the cluster center are morelikely to picked in subsequent iterations K-Medoids Algorithm More robust to outliers Dont update the cluster center with mean Use average dissimilarity to all other points in the cluster (i.e. medoid) \\(z_n^* = \\arg \\min d(x_n,\\mu_k)\\) \\(\\mu_k^* = \\arg \\min_n \\sum_{n&#39;} d(x_n,x_n&#39;)\\) Point has smallest sum of distances to all other points Partitioning around medoid Swap the current medoid center with a non-medoid to see if the cost decreases Selecting the number of Clusters Minimize Distortion Use a validation dataset Select the parameter that minimizes distortion on validation But usually it descreases monotonically with number of clusters Elbow method Rate at which distortion goes down with number of clusters Silhoutte Coefficient How similar object is to it’s own cluster compared to other clusters Measures Compactness Given data point i \\(a_i\\) = (Mean distance to observations in own cluster) \\(b_i\\) = (Mean Distance ot observations in the next closest cluster) \\(S_i = (a_i - b_i) / \\max(a_i, b_i)\\) Average the score for all the K clusters Ideal value is 1, worst value is -1 K-Means is a variant of EM K-Means assumes that clusters are spherical Hard assignment in K-Means vs Soft Assignment in EM 5.0.3 Spectral Clustering Clusters in a graph Find a subgraph Maxmimum number of within cluster connections Minimum number of between cluster connections Calculate degree and adjacency matrix Calculate the graph Laplacian \\(L=D-A\\) 0 if the nodes are not connected -1 if the nodes are connected Second smallest eigenvalue and eigenvector of L gives the best cut for graph partition The smallest value will be zero Group the nodes using second smallest eigenvector Applicable to pairwise similarity matrix Graph Representation Node is the object Distance denotes the edge 5.0.4 DBSCAN Density based spatial clustering Clusters are dense regions in space separated by regions with low density Recusrvely expand the cluster based on dense connectivity Can find clusters of arbitrary shape Two parameters: \\(\\epsilon\\) radius mininum # points to be contained in the neighbourhood Core Point Point that has mininum # points in \\(\\epsilon\\) radius Direct Density Reachble Points in \\(\\epsilon\\) radius of core point Density Reachable Chain connects the two points Chain is formed by considering many different core points Border Point Point is DDR but not core Expand the clusters recursively by collapsing DR and DDR points "],["support-vector-machines.html", "6 Support Vector Machines", " 6 Support Vector Machines 6.0.1 Linear SVM Classification setting Find the maximum-margin hyperplane that can separate the data Best hyperplane is the one that maximizes the margin Margin the distance of the hyperplane to closest data points from both classes Hyperplane: \\(H : wx +b = 0\\) Distance of a point (x) to a hyperplane (h): \\(d = \\frac{|Wx + b|}{||W||^2}\\) Margin is defined by the point closest to the hyperplane \\(\\gamma(W,b) = \\min_{x \\in D} \\frac{|Wx + b|}{||W||^2}\\) Margin is scale invariant SVM wants to maximize this margin For margin to be maximized, hyperplane must lie right in the middle of the two classes Otherwise it can be moved towards data points of the class that is further away and be further increased Mathematics Binary Classification \\(y_i \\in \\{+1,-1\\}\\) Need to find a separating hyperplane such that \\((Wx_i + b) &gt; 0 \\; \\forall \\; y_i = +1\\) \\((Wx_i + b) &lt; 0 \\; \\forall \\; y_i = -1\\) \\(y_i(Wx_i + b) \\ge 0\\) SVM posits that the best hyperplane is the one that maximizes the margin Margin acts as buffer which can lead to better generalization Objective \\(\\max_{W,b} \\gamma(W,b) \\; \\text{subject to} \\; y_i(Wx_i + b) &gt; 0\\) \\(\\max_{W,b} \\min_{x \\in D} \\frac{|Wx + b|}{||W||^2} \\; \\text{subject to} \\; y_i(Wx_i + b) &gt; 0\\) A max-min optimization problem Simplification The best possible hyperplace is scale invariant Add a constraint such that \\(|Wx +b| = 1\\) Updated objective \\(\\max \\frac{1}{||W||^2} \\; \\text{subject to} \\; y_i(Wx_i + b) \\ge 0 \\; ; |Wx +b| = 1\\) \\(\\min ||W||^2 \\; \\text{subject to} \\; y_i(Wx_i + b) \\ge 0 \\; ; |Wx +b| = 1\\) Combining the contraints \\(y_i(Wx_i + b) \\ge 0\\; ; |Wx +b| = 1 \\implies y_i(Wx_i + b) \\ge 1\\) Holds true because the objective is trying to minimize W Final objective \\(\\min ||W||^2 \\; \\text{subject to} \\; y_i(Wx_i + b) \\ge 1\\) Quadratic optimization problem Can be solved quickly unlike regression which involves inverting a large matrix Gives a unique solution unlike perceptron At the optimal solution, some training points will lie of the margin \\(y_i(Wx_i + b) = 1\\) These points are called support vectors Soft Constraints What if the optimization problem is infeasible? No solution exists Add relaxations i.e. allow for some misclassification Original: \\(y_i(Wx_i + b) \\ge 1\\) Relaxed: \\(y_i(Wx_i + b) \\ge 1 - \\xi_i \\; ; \\xi_i &gt; 0\\) \\(\\xi_i = \\begin{cases} 1 - y_i(Wx_i + b), &amp; \\text{if } y_i(Wx_i + b) &lt; 1\\\\0, &amp; \\text{otherwise} \\end{cases}\\) Hinge Loss \\(\\xi_i = \\max (1 - y_i(Wx_i + b), 0)\\) Objective: \\(\\min ||W||^2 + C \\sum_i \\max (1 - y_i(Wx_i + b), 0)\\) C is the regularization parameter that calculates trade-off High value of C allows for less torelance on errors Duality Primal problem is hard to solve Convert the problem to a Dual, which is easier to solve and also provides near-optimal solution to primal The gap is the optimality that arises in this process is the duality gap Lagrangian multipliers determine if strong suality exists Convert the above soft-margin SVM to dual via Lagrangian multipliers \\(\\sum \\alpha_i + \\sum\\sum \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\) \\(\\alpha\\) is the Lagrangian multiplier Kernelization Say the points are not separable in lower dimension Transform them via kernels to project them to a higher dimension The points may be separable the higher dimension Non-linear feature transformation Solve non-linear problems via Linear SVM Polynomial Kernel \\(K(x_i, x_j) = (x_i^T x_j + c)^d\\) The d regers to the degree of the polynomial Example: 2 points in 1-D (a and b) transformerd via second order polynomial kernel \\(K(a,b) = (ab + 1)^2 = 2ab+ a^2b^2 + 1 = (\\sqrt{2a}, a, 1)(\\sqrt{2b}, b, 1)\\) Calculates similarity between points in higher dimension RBF Kernel \\(K(x_i, x_j) = \\exp \\{\\gamma |x_i - x_j|^2\\}\\) The larger the distance between two observations, the less is the similarity Radial Kernel determines how much influence each observation has on classifying new data points Transforms points to an infinite dimension space Tayloy Expansion of exponential term shows how RBF is a polynomial function with inifnite dimensions 2 points in 1-D (a and b) transformerd via RBF \\(K(a,b) = (1, \\sqrt{\\frac{1}{1!}}a, \\sqrt{\\frac{1}{2!}}a^2...)(1, \\sqrt{\\frac{1}{1!}}b, \\sqrt{\\frac{1}{2!}}b^2...)\\) Kernel Trick Transforming the original dataset via Kernels and training SVM is expensive Convert Dot-products of support vectors to dot-products of mapping functions \\(x_i^T x_j \\implies \\phi(x_i)^T \\phi(x_j)\\) Kernels are chosen in a way that this is feasible SVM For Regression Margins should cover all data points (Hard) or most data points (Soft) The boundary now lies in the middle of the margins The regression model to estimate the target values The objective is to minimize the the distance of the points to the boundary Hard SVM is sensitive to outliers "],["dimensionality-reduction.html", "7 Dimensionality Reduction", " 7 Dimensionality Reduction 7.0.1 Background Curse of Dimensionality Data has too many features (n &lt;&lt; p) Data volume required for good generalization grows exponentially Same edge (say 10) square and cube 1x1 patch covers 1% area in quare 1x1x1 patch covers 0.1% volume in cube Two approaches Feature Selection Use only a subset of original features Latent Features Recombine the original features for more efficient representation Can be linear or non-linear 7.0.2 Principal Component Analysis Find a linear and orthogonal projection of data from high dimension to low dimension Encode original data \\(x \\in R^D\\) using \\(W \\in R^{D \\times L}\\) Encode: \\(z = W^T x \\in R^L\\) Decode \\(z\\) by projecting it from lower dimension to higher dimension Decode: \\(\\hat x = W z\\) Objective is to minimize reconstruction error \\(L(w) = {1 \\over N} \\sum ||x - \\hat x||^2\\) Proof: Project all the data to one dimension \\(w_1 \\in R^D\\) \\(\\hat x = z_{1} w_1\\) Optimal value of z and w that minimizes reconstruction error \\(L = {1 \\over N} \\sum ||x_i - z_{i1} w_1||^2\\) \\(L = {1 \\over N} \\sum (x_i - z_{i1} w_1)^T(x_i - z_{i1} w_1)\\) \\(L = {1 \\over N} \\sum x_i^T x_i -2 z_{i1} w_1^T x_i - z_{i1} w_1^Tw_1 z_{i1}\\) Orthonormal Assumption \\(\\implies w_1^Tw_1 = 1\\) \\(L = {1 \\over N} \\sum x_i^T x_i -2 z_{i1} w_1^T x_i - z_{i1}^2\\) Take Derivaties wrt z and w \\({\\delta L \\over \\delta z_{i1}} = {1 \\over N} (-2 w_1^T x_i + 2 z_{i1}) = 0\\) Optimal Embedding: \\(z_{i1} = w_1^T x\\) Plugging the value of z in L \\(L = {1 \\over N} \\sum x_i^T x_i - z_{i1}^2\\) \\(L = C - {1 \\over N} \\sum z_{i1}^2\\) \\(L = C - {1 \\over N} \\sum w_1^T x_i^T x_i w_1\\) \\(L = - {1 \\over N} w_1^T \\Sigma w_1\\) \\(\\Sigma\\) is the Var-Cov matrix of X The loss can be minimized trivially by scaling \\(w\\) To avoid this, impose a unit-norm constraint on \\(w\\) \\(L = {1 \\over N} w_1^T \\Sigma w_1 + \\lambda (w_1^T w_1 - 1)\\) \\({\\delta L \\over \\delta w_1} = -2 \\Sigma w_1 + 2 \\lambda w_1 = 0\\) Optimal w is given by eigen vector of \\(\\Sigma\\) To minimize the loss, pick the vector corresponding to highest eigenvalue PCA finds vectors that maximize the variance of projected data \\(L = C - {1 \\over N} \\sum z_{i1}^2\\) The original data is scaled \\(E(z_1) = E(w_1^T x) = 0\\) \\(L = C + \\text{Var}(z_1)\\) Geometric Explanation Find a new axis to capture the data Distance of the point from origin is fixed \\(R^2\\) \\(D^2\\) if the distance of the point from origin along the new axis (Variance) \\(\\epsilon\\) if the vertical distance of the point from the new axis (Distortion) By Pythagoras theorem \\(R^2 = D^2 + \\epsilon\\) PCA maximizes the variance \\(D^2\\) Is equivalent to minimizing distortion \\(\\epsilon\\) as \\(R^2\\) is constant Eigenvalues euqal the sum-sq(distances) on points on the principal component axis Use eigenvalues to understand how much variation is captured by each principal component Use scree plot (varation captured vs # components) to understand how many components should be included The maximum number of components are equal to the number of features in the original data Full basis If data is 2D, the eigen value for the 3rd PC will be 0 Principal components are linear combinations of original features The weights used for linear combinations are called factor loadings Factor loadings denote the importance of features in capturing variance PCA + linear regression is still interpretable Use estimated coefficients and factor loadings to understand how the original variables are being used PCA is calculated using SVD (singular value decomposition) \\(X = U S V^T \\in R^{N \\times D}\\) \\(U \\in R^{N \\times N}\\) is orthonormal \\(S \\in R^{N \\times D}\\) is diagonal \\(V \\in R^{D \\times D}\\) is orthonormal \\(X^T X = (U S V^{T})^T(U S V^{T}) = V(S^TS)V^T\\) Since S is a diagonal matrix, \\(S^TS\\) is diagonal as well \\(X^T X = VDV^T\\) On mutiplying both Sides by V: \\((X^T X)V = VD\\) D matrix gives the eigen values and V matrix gives the corresponding eigenvectors Notes PCA doesn’t work well if the interrelationships are non-linear Kernel PCA, Factor Analysis PCA doesn’t work well in case of outliers PCA can’t handle missing data PCA is unsupervised LDA is a supervised dimensionality reduction technique 7.0.3 Stochastic Neighbour Embedding (SNE) Unsupervised Non-parametric Mehtod for dimensionality reduction Manifold is a topological space which is locally Euclidean Eath is a 2D surface embedded in a 3D space High-dimensional data can lie in a low dimenison manifold Idea is to preserve nearest neighbours instead of preserving distances Convert the distances in high-dimension to probabilities Probability the point i will select j as it’s neighbour Gaussian Kernel \\(p_{j|i} \\propto \\exp({|| x_i - x_j||^2 \\over 2\\sigma_i^2})\\) \\(\\sigma_i^2\\) is the variance for data point i Magnify the scale of points in dense region Diminish the scale of points in sparse regions Perplexity parameter (say 30) Variance will be adjusted to cover approx 30 neighbours Balance between local and global aspects of the data Initialize the low-dimnesion representations and calculate the same probability \\(q_{j|i} \\propto \\exp({|| z_i - z_j||^2})\\) Variance is assumed to be constant here A good representation will preserve the neighbours \\(p\\) and \\(q\\) are probability distributions. KL Divergence will capture the distance between them \\(L = KL(p||q) = \\sum_i\\sum_j p_{i|j}\\log({p_{i|j} \\over q_{i|j}})\\) If p is high and q is low, the penalty is high Points were neighbours in high dimension but not in lo dimension If p is low and q is high, the penalty is low Unrelated points are pushed closer now Calculate \\(z\\) by minimizing KL-Div using SGD \\(\\Delta_{z_i} L = 0\\) \\(2 \\sum (z_i - z_j) (p_{i|j} - q_{i|j} + p_{j|i} - q_{j|i})\\) Symmetric SNE In the above formulation the distances are not symmetric \\(p_{i|j} \\ne p_{j|i}\\) To enforce this: \\(p_{ij} = (p_{i|j} + p_{j|i}) / 2\\) Equivalent to using constant variance in high-dimensional space \\(\\Delta_{z_i} L = 4 \\sum (z_i - z_j) (p_{ij} - q_{ij})\\) Similar to Potential energy in a spring (F = kx) \\((p_{ij} - q_{ij})\\) is k \\((z_i - z_j)\\) is x t-SNE SNE has a crowding problem Gaussian Kernel pushes moderately far away points in high dimension close together in low dimension (squared errors) Replace it with t-distribution that has fatter tails (probability goes to 0 slowly) The fatter tails allow dissimilar points to be far apart in lower dimension as well Removes unwanted attractive forces between points that are modelrately far in high dimension \\(q_{j|i} \\propto (1+{|| z_i - z_j||^2})^{-1}\\) \\(\\Delta_{z_i} L = \\sum (z_i - z_j) (p_{ij} - q_{ij}) (1 + || z_i - z_j||^2)^{-1}\\) \\((1 + || z_i - z_j||^2)^{-1}\\) ensures well separated clusters with tightly packed points inside Introduces strong repulsions between the dissimilar datapoints that are modeled by small pairwise distance in the low-dimensional map Coordinates after embedding have no inherent meaning UMAP Uniform Manifold Approximation and Projection Similar to t-SNE but much faster t-SNE calculates all pairwise distances UMAP calculates distances between close neighbours only t-SNE start with random initialization, UMAP start with spectral embeddings t-SNE moves every points slightly in each iteration, UMAP can move single points or subset of points in each iteration Mathematics t-SNE uses Gaussian desnity function to calculate the distance between points in high dimension UMAP uses similarity scores Hyperparameter: number of neighbours (similar to perplexity in t-SNE) Calculate log(number of neighbours) Calculate similarity scores \\(\\exp(-(\\text{raw distance} - \\text{distance to nearest neighbour}) / \\sigma\\) Rescale the curve such that sum of distances = log(number of neighbours) UMAP makes the scores symmetrical by \\((S_1 + S_2) - S_1S_2\\) Initialize a low dimension graph using Spectral Embedding Decompoistion of Graph Laplacian Graph Laplacian = Degree Matrix - Adjacency Matrix Calculate the similarity in low dimension using t-distrbution \\((1 + \\alpha d^{2\\beta})^{-1}\\) The parameters help user control the shape of the curve Cost Function Cross-Entropy between graphs \\(\\log(1 - S_{\\text{not neighbour}}) - log(S_{\\text{neighbour}})\\) UMAP can accomodate new data (predict function) without recomputation "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
