# Linear Regression

### Bi-variate Regression

- Fit a straight line to data
    - How the conditional mean of response variable changes with change in explanatory variables
    - Minimize SSE: $\sum (y - \hat y)^2$ 
    - $E(y|x) = \hat y = a + bx$
    - $a = \bar y - b \bar x$
    - $b = S_{xy} / S_{xx} = \sum (y-\bar y)(x - \bar x) / \sum (x - \bar x)^2$
- Outlier: If a point lies far off from the rest of the data
- Influential: If a point causes large change in slope of the fitted line
- Variance
    - Assume constant variance (homoscedasticity)
    - $s = \sqrt{SSE \over (n-p)}$
- Correlation
    - Strength of linear association
    - $r = \frac{\sum{(x - \bar x)(y - \bar y)}}{\sqrt{(x - \bar x)^2} \sqrt{(y - \bar y)^2}}$
    - $r = (s_x / s_y) b$
        - In case of standardized variables, r = b
        - Regression towards mean
            - $|r| <= 1$
            - Line passes thourgh $(\bar x, \bar y)$
            - A unit increase in x leads to r increase in y
- R-Square
    - Coefficient of determination 
    - $R^2 = (TSS - SSE) / TSS$
    - $SSE = \sum(y - \hat y)^2$
    - $TSS = \sum(y - \bar y)^2$
    - Sqaured of correlation coefficient
- Statistical Significance
    - $t = b / se$ 
    - $se = s / \sqrt{S_{xx}}$
    - $s = \sqrt{SSE / (n-2)}$
    - $t^2 = F = \frac{r^2 / (2-1)}{1-r^2 / n - 2}$

### Multivariate Regression

- Types of relationships
    - Spurious: Both variables jointly affected by a third variable
    - Mediator: An intervening third variable indirectly affects the two variables
    - Suppressor: Association only exists after controlling for a third variable
- Extending regression to multiple explanatory variables
    - $E(y|x) = \hat y = a + b_1 x_1 + b_2 x_2$ 
    - b1 is the relationship between y and x1 after controlling for all other variables (x2)
    - b1 is the partial regression coefficient
    - It represents first order partial correlation 
    - $r_{yx_1.x_2}= (R^2 - r^2_{yx_2}) / (1 - r^2_{yx_2})$
- Partial Regression Plots
    - True association between x1 and y after controlling for x2
    - Regress y on x2: $\hat y = a' + b'x_2$
    - Regress x1 on x2: $\hat x_1 = a'' + b'' x_2$
    - Plot the residuals from first regression against the second.
- Statistical Significance
    - Collective Influence
        - F Test
        - $F = \frac{R^2 / p-1}{(1 - R^2) / (n-p)}$
    - Individual Influence
        - t test
        - $t = \beta / se$
        - $s = \sqrt{SSE / n-p}$
    - Comparing Two Models
        - Complete Model: With all the variables 
        - Reduced Model: Dropping some of the variables
        - $F = \frac{(SSE_r - SSE_c)/(df_c - df_r)}{SSE_c / df_c}$
- ANOVA
    - Total $SST = \sum (y - \bar y)^2, \; df = n-1$ 
    - Regression $SSR = \sum (\hat y - \bar y)^2, \; df = p-1$ 
    - Error $SSE = \sum (y - \hat y)^2, \; df = n-p$ 
    - $F = MSR / MSE = (SSR / df) / (SSE / df)$
- Bonferroni Correction
    - Multiple comparisons
    - Significance Level // # of comparisons
- 