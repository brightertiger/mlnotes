<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Dimensionality Reduction | Notes on ML</title>
  <meta name="description" content="7 Dimensionality Reduction | Notes on ML" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Dimensionality Reduction | Notes on ML" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Dimensionality Reduction | Notes on ML" />
  
  
  

<meta name="author" content="brightertiger" />


<meta name="date" content="2022-03-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="support-vector-machines.html"/>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ML Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html"><i class="fa fa-check"></i><b>2</b> Decision Trees &amp; Random Forests</a>
<ul>
<li class="chapter" data-level="2.0.1" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#decision-trees"><i class="fa fa-check"></i><b>2.0.1</b> Decision Trees</a></li>
<li class="chapter" data-level="2.0.2" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#splitting"><i class="fa fa-check"></i><b>2.0.2</b> Splitting</a></li>
<li class="chapter" data-level="2.0.3" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.0.3</b> Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="2.0.4" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#nature-of-decision-trees"><i class="fa fa-check"></i><b>2.0.4</b> Nature of Decision Trees</a></li>
<li class="chapter" data-level="2.0.5" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#bagging"><i class="fa fa-check"></i><b>2.0.5</b> Bagging</a></li>
<li class="chapter" data-level="2.0.6" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#random-forest"><i class="fa fa-check"></i><b>2.0.6</b> Random Forest</a></li>
<li class="chapter" data-level="2.0.7" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#extratrees"><i class="fa fa-check"></i><b>2.0.7</b> ExtraTrees</a></li>
<li class="chapter" data-level="2.0.8" data-path="decision-trees-random-forests.html"><a href="decision-trees-random-forests.html#variable-importance"><i class="fa fa-check"></i><b>2.0.8</b> Variable Importance</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>3</b> Boosting</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="boosting.html"><a href="boosting.html#overview"><i class="fa fa-check"></i><b>3.0.1</b> Overview</a></li>
<li class="chapter" data-level="3.0.2" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>3.0.2</b> Gradient Boosting</a></li>
<li class="chapter" data-level="3.0.3" data-path="boosting.html"><a href="boosting.html#adaboost-for-classification"><i class="fa fa-check"></i><b>3.0.3</b> Adaboost for Classification</a></li>
<li class="chapter" data-level="3.0.4" data-path="boosting.html"><a href="boosting.html#notes"><i class="fa fa-check"></i><b>3.0.4</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>4</b> XGBoost</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="xgboost.html"><a href="xgboost.html#mathematical-details"><i class="fa fa-check"></i><b>4.0.1</b> Mathematical Details</a></li>
<li class="chapter" data-level="4.0.2" data-path="xgboost.html"><a href="xgboost.html#regression"><i class="fa fa-check"></i><b>4.0.2</b> Regression</a></li>
<li class="chapter" data-level="4.0.3" data-path="xgboost.html"><a href="xgboost.html#classification"><i class="fa fa-check"></i><b>4.0.3</b> Classification</a></li>
<li class="chapter" data-level="4.0.4" data-path="xgboost.html"><a href="xgboost.html#optimizations"><i class="fa fa-check"></i><b>4.0.4</b> Optimizations</a></li>
<li class="chapter" data-level="4.0.5" data-path="xgboost.html"><a href="xgboost.html#comparisons"><i class="fa fa-check"></i><b>4.0.5</b> Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>5</b> Clustering</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="clustering.html"><a href="clustering.html#hierarchical-agglomerative-clustering"><i class="fa fa-check"></i><b>5.0.1</b> Hierarchical Agglomerative Clustering</a></li>
<li class="chapter" data-level="5.0.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>5.0.2</b> K-Means Clustering</a></li>
<li class="chapter" data-level="5.0.3" data-path="clustering.html"><a href="clustering.html#spectral-clustering"><i class="fa fa-check"></i><b>5.0.3</b> Spectral Clustering</a></li>
<li class="chapter" data-level="5.0.4" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>5.0.4</b> DBSCAN</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>6</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linear-svm"><i class="fa fa-check"></i><b>6.0.1</b> Linear SVM</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>7</b> Dimensionality Reduction</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#background"><i class="fa fa-check"></i><b>7.0.1</b> Background</a></li>
<li class="chapter" data-level="7.0.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principal-component-analysis"><i class="fa fa-check"></i><b>7.0.2</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="7.0.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#stochastic-neighbour-embedding-sne"><i class="fa fa-check"></i><b>7.0.3</b> Stochastic Neighbour Embedding (SNE)</a></li>
</ul></li>
<li class="divider"></li>
<li>Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on ML</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dimensionality-reduction" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Dimensionality Reduction</h1>
<div id="background" class="section level3" number="7.0.1">
<h3><span class="header-section-number">7.0.1</span> Background</h3>
<ul>
<li>Curse of Dimensionality
<ul>
<li>Data has too many features (n &lt;&lt; p)</li>
<li>Data volume required for good generalization grows exponentially</li>
<li>Same edge (say 10) square and cube
<ul>
<li>1x1 patch covers 1% area in quare</li>
<li>1x1x1 patch covers 0.1% volume in cube</li>
</ul></li>
</ul></li>
<li>Two approaches
<ul>
<li>Feature Selection
<ul>
<li>Use only a subset of original features<br />
</li>
</ul></li>
<li>Latent Features
<ul>
<li>Recombine the original features for more efficient representation</li>
<li>Can be linear or non-linear</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="principal-component-analysis" class="section level3" number="7.0.2">
<h3><span class="header-section-number">7.0.2</span> Principal Component Analysis</h3>
<ul>
<li>Find a linear and orthogonal projection of data from high dimension to low dimension
<ul>
<li>Encode original data <span class="math inline">\(x \in R^D\)</span> using <span class="math inline">\(W \in R^{D \times L}\)</span>
<ul>
<li>Encode: <span class="math inline">\(z = W^T x \in R^L\)</span></li>
</ul></li>
<li>Decode <span class="math inline">\(z\)</span> by projecting it from lower dimension to higher dimension
<ul>
<li>Decode: <span class="math inline">\(\hat x = W z\)</span></li>
</ul></li>
</ul></li>
<li>Objective is to minimize reconstruction error
<ul>
<li><span class="math inline">\(L(w) = {1 \over N} \sum ||x - \hat x||^2\)</span></li>
</ul></li>
<li>Proof: Project all the data to one dimension
<ul>
<li><span class="math inline">\(w_1 \in R^D\)</span></li>
<li><span class="math inline">\(\hat x = z_{1} w_1\)</span></li>
<li>Optimal value of z and w that minimizes reconstruction error</li>
<li><span class="math inline">\(L = {1 \over N} \sum ||x_i - z_{i1} w_1||^2\)</span></li>
<li><span class="math inline">\(L = {1 \over N} \sum (x_i - z_{i1} w_1)^T(x_i - z_{i1} w_1)\)</span></li>
<li><span class="math inline">\(L = {1 \over N} \sum x_i^T x_i -2 z_{i1} w_1^T x_i - z_{i1} w_1^Tw_1 z_{i1}\)</span></li>
<li>Orthonormal Assumption <span class="math inline">\(\implies w_1^Tw_1 = 1\)</span></li>
<li><span class="math inline">\(L = {1 \over N} \sum x_i^T x_i -2 z_{i1} w_1^T x_i - z_{i1}^2\)</span></li>
<li>Take Derivaties wrt z and w</li>
<li><span class="math inline">\({\delta L \over \delta z_{i1}} = {1 \over N} (-2 w_1^T x_i + 2 z_{i1}) = 0\)</span></li>
<li>Optimal Embedding: <span class="math inline">\(z_{i1} = w_1^T x\)</span></li>
<li>Plugging the value of z in L</li>
<li><span class="math inline">\(L = {1 \over N} \sum x_i^T x_i - z_{i1}^2\)</span></li>
<li><span class="math inline">\(L = C - {1 \over N} \sum z_{i1}^2\)</span></li>
<li><span class="math inline">\(L = C - {1 \over N} \sum w_1^T x_i^T x_i w_1\)</span></li>
<li><span class="math inline">\(L = - {1 \over N} w_1^T \Sigma w_1\)</span></li>
<li><span class="math inline">\(\Sigma\)</span> is the Var-Cov matrix of X</li>
<li>The loss can be minimized trivially by scaling <span class="math inline">\(w\)</span></li>
<li>To avoid this, impose a unit-norm constraint on <span class="math inline">\(w\)</span></li>
<li><span class="math inline">\(L = {1 \over N} w_1^T \Sigma w_1 + \lambda (w_1^T w_1 - 1)\)</span></li>
<li><span class="math inline">\({\delta L \over \delta w_1} = -2 \Sigma w_1 + 2 \lambda w_1 = 0\)</span></li>
<li>Optimal w is given by eigen vector of <span class="math inline">\(\Sigma\)</span></li>
<li>To minimize the loss, pick the vector corresponding to highest eigenvalue</li>
</ul></li>
<li>PCA finds vectors that maximize the variance of projected data
<ul>
<li><span class="math inline">\(L = C - {1 \over N} \sum z_{i1}^2\)</span></li>
<li>The original data is scaled</li>
<li><span class="math inline">\(E(z_1) = E(w_1^T x) = 0\)</span></li>
<li><span class="math inline">\(L = C + \text{Var}(z_1)\)</span></li>
</ul></li>
<li>Geometric Explanation
<ul>
<li>Find a new axis to capture the data</li>
<li>Distance of the point from origin is fixed <span class="math inline">\(R^2\)</span></li>
<li><span class="math inline">\(D^2\)</span> if the distance of the point from origin along the new axis (Variance)</li>
<li><span class="math inline">\(\epsilon\)</span> if the vertical distance of the point from the new axis (Distortion)</li>
<li>By Pythagoras theorem <span class="math inline">\(R^2 = D^2 + \epsilon\)</span></li>
<li>PCA maximizes the variance <span class="math inline">\(D^2\)</span></li>
<li>Is equivalent to minimizing distortion <span class="math inline">\(\epsilon\)</span> as <span class="math inline">\(R^2\)</span> is constant</li>
</ul></li>
<li>Eigenvalues euqal the sum-sq(distances) on points on the principal component axis</li>
<li>Use eigenvalues to understand how much variation is captured by each principal component</li>
<li>Use scree plot (varation captured vs # components) to understand how many components should be included</li>
<li>The maximum number of components are equal to the number of features in the original data
<ul>
<li>Full basis</li>
<li>If data is 2D, the eigen value for the 3rd PC will be 0</li>
</ul></li>
<li>Principal components are linear combinations of original features
<ul>
<li>The weights used for linear combinations are called factor loadings</li>
<li>Factor loadings denote the importance of features in capturing variance</li>
</ul></li>
<li>PCA + linear regression is still interpretable
<ul>
<li>Use estimated coefficients and factor loadings to understand how the original variables are being used</li>
</ul></li>
<li>PCA is calculated using SVD (singular value decomposition)
<ul>
<li><span class="math inline">\(X = U S V^T \in R^{N \times D}\)</span>
<ul>
<li><span class="math inline">\(U \in R^{N \times N}\)</span> is orthonormal</li>
<li><span class="math inline">\(S \in R^{N \times D}\)</span> is diagonal</li>
<li><span class="math inline">\(V \in R^{D \times D}\)</span> is orthonormal</li>
</ul></li>
<li><span class="math inline">\(X^T X = (U S V^{T})^T(U S V^{T}) = V(S^TS)V^T\)</span></li>
<li>Since S is a diagonal matrix, <span class="math inline">\(S^TS\)</span> is diagonal as well</li>
<li><span class="math inline">\(X^T X = VDV^T\)</span></li>
<li>On mutiplying both Sides by V: <span class="math inline">\((X^T X)V = VD\)</span></li>
<li>D matrix gives the eigen values and V matrix gives the corresponding eigenvectors</li>
</ul></li>
<li>Notes
<ul>
<li>PCA doesn’t work well if the interrelationships are non-linear
<ul>
<li>Kernel PCA, Factor Analysis</li>
</ul></li>
<li>PCA doesn’t work well in case of outliers</li>
<li>PCA can’t handle missing data</li>
<li>PCA is unsupervised
<ul>
<li>LDA is a supervised dimensionality reduction technique</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="stochastic-neighbour-embedding-sne" class="section level3" number="7.0.3">
<h3><span class="header-section-number">7.0.3</span> Stochastic Neighbour Embedding (SNE)</h3>
<ul>
<li>Unsupervised Non-parametric Mehtod for dimensionality reduction</li>
<li>Manifold is a topological space which is locally Euclidean
<ul>
<li>Eath is a 2D surface embedded in a 3D space</li>
<li>High-dimensional data can lie in a low dimenison manifold</li>
</ul></li>
<li>Idea is to preserve nearest neighbours instead of preserving distances</li>
<li>Convert the distances in high-dimension to probabilities
<ul>
<li>Probability the point i will select j as it’s neighbour</li>
<li>Gaussian Kernel</li>
<li><span class="math inline">\(p_{j|i} \propto \exp({|| x_i - x_j||^2 \over 2\sigma_i^2})\)</span></li>
<li><span class="math inline">\(\sigma_i^2\)</span> is the variance for data point i
<ul>
<li>Magnify the scale of points in dense region</li>
<li>Diminish the scale of points in sparse regions</li>
<li>Perplexity parameter (say 30)</li>
<li>Variance will be adjusted to cover approx 30 neighbours</li>
<li>Balance between local and global aspects of the data</li>
</ul></li>
</ul></li>
<li>Initialize the low-dimnesion representations and calculate the same probability
<ul>
<li><span class="math inline">\(q_{j|i} \propto \exp({|| z_i - z_j||^2})\)</span></li>
<li>Variance is assumed to be constant here</li>
</ul></li>
<li>A good representation will preserve the neighbours</li>
<li><span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are probability distributions. KL Divergence will capture the distance between them</li>
<li><span class="math inline">\(L = KL(p||q) = \sum_i\sum_j p_{i|j}\log({p_{i|j} \over q_{i|j}})\)</span>
<ul>
<li>If p is high and q is low, the penalty is high</li>
<li>Points were neighbours in high dimension but not in lo dimension</li>
<li>If p is low and q is high, the penalty is low</li>
<li>Unrelated points are pushed closer now</li>
</ul></li>
<li>Calculate <span class="math inline">\(z\)</span> by minimizing KL-Div using SGD
<ul>
<li><span class="math inline">\(\Delta_{z_i} L = 0\)</span></li>
<li><span class="math inline">\(2 \sum (z_i - z_j) (p_{i|j} - q_{i|j} + p_{j|i} - q_{j|i})\)</span></li>
</ul></li>
<li>Symmetric SNE
<ul>
<li>In the above formulation the distances are not symmetric</li>
<li><span class="math inline">\(p_{i|j} \ne p_{j|i}\)</span></li>
<li>To enforce this: <span class="math inline">\(p_{ij} = (p_{i|j} + p_{j|i}) / 2\)</span></li>
<li>Equivalent to using constant variance in high-dimensional space</li>
<li><span class="math inline">\(\Delta_{z_i} L = 4 \sum (z_i - z_j) (p_{ij} - q_{ij})\)</span>
<ul>
<li>Similar to Potential energy in a spring (F = kx)</li>
<li><span class="math inline">\((p_{ij} - q_{ij})\)</span> is k</li>
<li><span class="math inline">\((z_i - z_j)\)</span> is x</li>
</ul></li>
</ul></li>
<li>t-SNE
<ul>
<li>SNE has a crowding problem</li>
<li>Gaussian Kernel pushes moderately far away points in high dimension close together in low dimension (squared errors)</li>
<li>Replace it with t-distribution that has fatter tails (probability goes to 0 slowly)
<ul>
<li>The fatter tails allow dissimilar points to be far apart in lower dimension as well</li>
<li>Removes unwanted attractive forces between points that are modelrately far in high dimension</li>
</ul></li>
<li><span class="math inline">\(q_{j|i} \propto (1+{|| z_i - z_j||^2})^{-1}\)</span></li>
<li><span class="math inline">\(\Delta_{z_i} L = \sum (z_i - z_j) (p_{ij} - q_{ij}) (1 + || z_i - z_j||^2)^{-1}\)</span></li>
<li><span class="math inline">\((1 + || z_i - z_j||^2)^{-1}\)</span> ensures well separated clusters with tightly packed points inside</li>
<li>Introduces strong repulsions between the dissimilar datapoints that are modeled by small pairwise distance in the low-dimensional map</li>
<li>Coordinates after embedding have no inherent meaning</li>
</ul></li>
<li>UMAP
<ul>
<li>Uniform Manifold Approximation and Projection<br />
</li>
<li>Similar to t-SNE but much faster
<ul>
<li>t-SNE calculates all pairwise distances</li>
<li>UMAP calculates distances between close neighbours only</li>
</ul></li>
<li>t-SNE start with random initialization, UMAP start with spectral embeddings</li>
<li>t-SNE moves every points slightly in each iteration, UMAP can move single points or subset of points in each iteration</li>
<li>Mathematics
<ul>
<li>t-SNE uses Gaussian desnity function to calculate the distance between points in high dimension</li>
<li>UMAP uses similarity scores
<ul>
<li>Hyperparameter: number of neighbours (similar to perplexity in t-SNE)</li>
<li>Calculate log(number of neighbours)</li>
<li>Calculate similarity scores</li>
<li><span class="math inline">\(\exp(-(\text{raw distance} - \text{distance to nearest neighbour}) / \sigma\)</span></li>
<li>Rescale the curve such that sum of distances = log(number of neighbours)</li>
</ul></li>
<li>UMAP makes the scores symmetrical by <span class="math inline">\((S_1 + S_2) - S_1S_2\)</span></li>
<li>Initialize a low dimension graph using Spectral Embedding
<ul>
<li>Decompoistion of Graph Laplacian</li>
<li>Graph Laplacian = Degree Matrix - Adjacency Matrix</li>
</ul></li>
<li>Calculate the similarity in low dimension using t-distrbution
<ul>
<li><span class="math inline">\((1 + \alpha d^{2\beta})^{-1}\)</span></li>
<li>The parameters help user control the shape of the curve</li>
</ul></li>
<li>Cost Function
<ul>
<li>Cross-Entropy between graphs</li>
<li><span class="math inline">\(\log(1 - S_{\text{not neighbour}}) - log(S_{\text{neighbour}})\)</span><br />
</li>
</ul></li>
</ul></li>
<li>UMAP can accomodate new data (predict function) without recomputation</li>
</ul></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="support-vector-machines.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
